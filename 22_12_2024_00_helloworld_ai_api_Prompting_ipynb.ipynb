{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNRAX0RcorKnvkWggj4hGMK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CrystleYS/Q2-Lecture-4/blob/main/22_12_2024_00_helloworld_ai_api_Prompting_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-p_9M1NvI9W"
      },
      "outputs": [],
      "source": [
        "#This notebook contains examples of how to write and run your first prompts with the Gemini API.\n",
        "\n",
        "!pip install -U -q \"google-generativeai>=0.7.2\" # Install the Python SDK\n",
        "\n",
        "\n",
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "XFEuFlSjdR4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run your first prompt\n",
        "#Use the generate_content method to generate responses to your prompts. You can pass text directly to generate_content, and use the .text property to get the text content of the response.\n",
        "\n",
        "\n",
        "model = genai.GenerativeModel('gemini-2.0-flash-exp')\n",
        "response = model.generate_content(\"Give me python code to sort a list\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "UE9BNJW_dWDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run your first prompt\n",
        "#Use the generate_content method to generate responses to your prompts. You can pass text directly to generate_content, and use the .text property to get the text content of the response.\n",
        "\n",
        "\n",
        "model = genai.GenerativeModel('gemini-2.0-flash-exp')\n",
        "response = model.generate_content(\"HI\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "7-GXxfedg7Cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run your first prompt\n",
        "#Use the generate_content method to generate responses to your prompts. You can pass text directly to generate_content, and use the .text property to get the text content of the response.\n",
        "\n",
        "\n",
        "model = genai.GenerativeModel('gemini-2.0-flash-exp')\n",
        "response = model.generate_content(\"Hi, My name is Muhammad Younas\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "4lU-Jm6dhAT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response\n"
      ],
      "metadata": {
        "id": "ydPayV2qhjIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.candidates [0].content.parts[0].text\n"
      ],
      "metadata": {
        "id": "SHw7e-a5iNpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response\n"
      ],
      "metadata": {
        "id": "V55C_5FCi7A-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\"what is my name?\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "XRGbkxMVjM2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\"Hi, My name Muhammad Younas\")\n",
        "print(response.text)\n",
        "response = model.generate_content(\"what is my name?\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "eWQrgZX0m5rY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\"what is my name?\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "hnUzi9CMnStX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P6MzzmM5u73f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use images in your prompt:**\n",
        "\n",
        "Here you will download an image from a URL and pass that image in our prompt.\n",
        "\n",
        "First, you download the image and load it with PIL:"
      ],
      "metadata": {
        "id": "KDSevOimokRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -o image.jpg \"https://storage.googleapis.com/generativeai-downloads/images/jetpack.jpg\"\n"
      ],
      "metadata": {
        "id": "f78p63Agot2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL.Image\n",
        "img = PIL.Image.open('image.jpg')\n",
        "img"
      ],
      "metadata": {
        "id": "NZ7-WsFYpSvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL.Image\n",
        "img = PIL.Image.open('/content/image.jpg')\n",
        "img"
      ],
      "metadata": {
        "id": "u7YbxeccpeSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"This image contains a sketch of a potential product along with some notes. Given the product sketch, describe the product as thoroughly as possible based on what you see in the image, making sure to note all of the product features. Return output in json format: {description: description, features: [feature1, feature2, feature3, etc]}\"\"\""
      ],
      "metadata": {
        "id": "PNeJVP3Tp49y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4 Steps Promp Engineering\n",
        "1. Persona\n",
        "2. Context\n",
        "3. Task\n",
        "4. Output\n",
        "**1.Instruction** - a specific task or instruction you want the model to perform\n",
        "\n",
        "**2. Context** - external information or additional context that can steer the model to better responses\n",
        "\n",
        "**3. Input Data** - the input or question that we are interested to find a response for\n",
        "\n",
        "**4. Output Indicator** - the type or format of the output."
      ],
      "metadata": {
        "id": "LaJmYIaxqS9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel('gemini-2.0-flash-exp')\n",
        "response = model.generate_content([prompt, img])\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "0ewbz3VkrnGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Have a chat**\n",
        "\n",
        "The Gemini API enables you to have freeform conversations across multiple turns.\n",
        "\n",
        "The ChatSession class will store the conversation history for multi-turn interactions."
      ],
      "metadata": {
        "id": "I_7bEFKwt85I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel('gemini-2.0-flash-exp')\n",
        "chat = model.start_chat(history=[])"
      ],
      "metadata": {
        "id": "P1b6G_IeuHpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message(\"Hi, My name is Muhammad Younas\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "aZlfMZl0ucDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat.history)"
      ],
      "metadata": {
        "id": "JWyvoVsZurOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message(\"Hi, What is my name?\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "GkLx-ztlu-dL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set the temperature**\n",
        "\n",
        "Every prompt you send to the model includes parameters that control how the model generates responses. Use a genai.GenerationConfig to set these, or omit it to use the defaults.\n",
        "\n",
        "Temperature controls the degree of randomness in token selection. Use higher values for more creative responses, and lower values for more deterministic responses.\n",
        "\n",
        "You can set the generation_config when creating the model."
      ],
      "metadata": {
        "id": "f87K1ttvxYIf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QbCEINHdyBRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\n",
        "    'gemini-2.0-flash-exp',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        max_output_tokens=2000,\n",
        "        temperature=0.9,\n",
        "    ))"
      ],
      "metadata": {
        "id": "Xy31CaR7xfUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or, set the generation_config on an individual call to generate_content. Any values set there override values on the model constructor.\n",
        "\n",
        "Note: Although you can set the candidate_count in the generation_config, gemini-pro models will only return a single candidate at the this time."
      ],
      "metadata": {
        "id": "Uy5QC9rxx7Oy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    'Give me a numbered list of cat facts.',\n",
        "    # Limit to 5 facts.\n",
        "    generation_config = genai.GenerationConfig(stop_sequences=['\\n6'])\n",
        ")\n",
        ""
      ],
      "metadata": {
        "id": "OFwtHsjeyF4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.text)"
      ],
      "metadata": {
        "id": "5fuwfc_byRJM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}